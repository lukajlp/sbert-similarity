{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configs e Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# instalar para o kaggle\n",
    "%pip install sentence-transformers datasets accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%pip install -q --upgrade wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# BIBLIOTECAS DE ANÁLISE DE DADOS E NUMÉRICAS\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# BIBLIOTECAS DE VISUALIZAÇÃO DE DADOS\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# BIBLIOTECAS DE MACHINE LEARNING E ESTATÍSTICA\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# BIBLIOTECA PRINCIPAL DE DEEP LEARNING (PYTORCH)\n",
    "import torch\n",
    "\n",
    "# BIBLIOTECA DE NLP (SENTENCE TRANSFORMERS)\n",
    "from sentence_transformers import (\n",
    "    InputExample,  # Estrutura para encapsular um exemplo de treino (par de frases + label).\n",
    "    SentenceTransformer,  # Classe principal para carregar e usar modelos SBERT.\n",
    "    losses,  # Módulo com as funções de perda (ex: CosineSimilarityLoss).\n",
    "    util,  # Funções utilitárias (ex: util.cos_sim para similaridade).\n",
    ")\n",
    "from sentence_transformers.evaluation import (\n",
    "    EmbeddingSimilarityEvaluator,\n",
    ")  # Classe para avaliar a performance durante o treino.\n",
    "from sentence_transformers.similarity_functions import (\n",
    "    SimilarityFunction,\n",
    ")  # Enum para funções de similaridade (ex: COSINE).\n",
    "from sentence_transformers.trainer import (\n",
    "    SentenceTransformerTrainer,\n",
    ")  # API de alto nível para gerenciar o treinamento.\n",
    "from sentence_transformers.training_args import (\n",
    "    SentenceTransformerTrainingArguments,\n",
    ")  # Argumentos e configurações para o Trainer.\n",
    "\n",
    "# Define se o código rodará em GPU (cuda) ou CPU, otimizando a performance.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T21:28:55.025464Z",
     "iopub.status.busy": "2025-10-04T21:28:55.025154Z",
     "iopub.status.idle": "2025-10-04T21:28:55.149539Z",
     "shell.execute_reply": "2025-10-04T21:28:55.148843Z",
     "shell.execute_reply.started": "2025-10-04T21:28:55.025438Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "import wandb\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "wandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "wandb.login(key=wandb_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração do WandB Sweep para Hyperparameter Tuning\n",
    "sweep_config = {\n",
    "    \"method\": \"bayes\",  # ou 'grid', 'random'\n",
    "    \"metric\": {\"name\": \"eval_validation_spearman_cosine\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"learning_rate\": {\n",
    "            \"distribution\": \"log_uniform_values\",\n",
    "            \"min\": 1e-6,\n",
    "            \"max\": 5e-4,\n",
    "        },\n",
    "        \"num_train_epochs\": {\"values\": [2, 3, 4, 5, 6]},\n",
    "        \"per_device_train_batch_size\": {\"values\": [8, 16, 32]},\n",
    "        \"warmup_ratio\": {\"distribution\": \"uniform\", \"min\": 0.05, \"max\": 0.2},\n",
    "        \"weight_decay\": {\n",
    "            \"distribution\": \"log_uniform_values\",\n",
    "            \"min\": 1e-6,\n",
    "            \"max\": 1e-1,\n",
    "        },\n",
    "        \"model_name\": {\n",
    "            \"values\": [\n",
    "                \"all-MiniLM-L6-v2\",\n",
    "                \"all-mpnet-base-v2\",\n",
    "                \"paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "                \"distilbert-base-multilingual-cased\",\n",
    "            ]\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"Configuração do sweep definida!\")\n",
    "print(\"Modelos disponíveis para teste:\")\n",
    "for model in sweep_config[\"parameters\"][\"model_name\"][\"values\"]:\n",
    "    print(f\"  - {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "caminho_train = \"/kaggle/input/2025-2-similaridade-de-sentencas/train.csv\"\n",
    "caminho_test = \"/kaggle/input/2025-2-similaridade-de-sentencas/test.csv\"\n",
    "\n",
    "df = pd.read_csv(caminho_train)\n",
    "print(\"Arquivo train.csv carregado com sucesso!\")\n",
    "\n",
    "# Exibir as primeiras linhas e informações básicas\n",
    "print(\"\\nDimensões do DataFrame:\", df.shape)\n",
    "print(\"\\nColunas disponíveis:\", df.columns.tolist())\n",
    "print(\"\\nPrimeiras 3 linhas do DataFrame:\")\n",
    "print(df.head(3))\n",
    "\n",
    "print(\"\\nValores nulos por coluna:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Análise de Balanceamento das Classes\n",
    "print(\"\\nDistribuição das classes:\")\n",
    "print(df[\"similarity_score\"].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nEstatísticas do similarity_score:\")\n",
    "print(df[\"similarity_score\"].describe())\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df[\"similarity_score\"], bins=20, alpha=0.7)\n",
    "plt.title(\"Distribuição dos Scores de Similaridade\")\n",
    "plt.xlabel(\"Similarity Score\")\n",
    "plt.ylabel(\"Frequência\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(y=df[\"similarity_score\"])\n",
    "plt.title(\"Boxplot dos Scores de Similaridade\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparando Dados para o SBERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Separar X e y\n",
    "X = df[[\"sentence1\", \"sentence2\"]]\n",
    "y = df[\"similarity_score\"]\n",
    "\n",
    "# Separar dados em treino e validação\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalizar os scores para o intervalo [0, 1]\n",
    "y_train_norm = y_train / 5.0\n",
    "y_val_norm = y_val / 5.0\n",
    "\n",
    "# Verificar os resultados\n",
    "print(\"\\nDimensões do conjunto de treino:\", X_train.shape)\n",
    "print(\"Dimensões do conjunto de validação:\", X_val.shape)\n",
    "print(\"\\nDistribuição das classes no conjunto de treino:\")\n",
    "print(y_train.value_counts().sort_index())\n",
    "print(\"\\nDistribuição das classes no conjunto de validação:\")\n",
    "print(y_val.value_counts().sort_index())\n",
    "\n",
    "# Preparar dados como Dataset do Hugging Face\n",
    "train_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"sentence1\": X_train[\"sentence1\"].tolist(),\n",
    "        \"sentence2\": X_train[\"sentence2\"].tolist(),\n",
    "        \"label\": y_train_norm.tolist(),\n",
    "    }\n",
    ")\n",
    "\n",
    "val_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"sentence1\": X_val[\"sentence1\"].tolist(),\n",
    "        \"sentence2\": X_val[\"sentence2\"].tolist(),\n",
    "        \"label\": y_val_norm.tolist(),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Preparando Dados para o SBERT\n",
    "train_examples = [\n",
    "    InputExample(texts=[s1, s2], label=float(label))\n",
    "    for s1, s2, label in zip(X_train[\"sentence1\"], X_train[\"sentence2\"], y_train_norm)\n",
    "]\n",
    "\n",
    "val_examples = [\n",
    "    InputExample(texts=[s1, s2], label=float(label))\n",
    "    for s1, s2, label in zip(X_val[\"sentence1\"], X_val[\"sentence2\"], y_val)\n",
    "]\n",
    "\n",
    "print(f\"\\nNúmero de exemplos de treino: {len(train_examples)}\")\n",
    "print(f\"Número de exemplos de validação: {len(val_examples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuração do Modelo SBERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Função para configurar e treinar o modelo\n",
    "def train_model_with_config(config=None):\n",
    "    \"\"\"\n",
    "    Função para treinar o modelo com configurações específicas\n",
    "    Pode ser usada tanto para treinamento único quanto para sweep\n",
    "    \"\"\"\n",
    "\n",
    "    # Configurações padrão (caso não seja sweep)\n",
    "    default_config = {\n",
    "        \"model_name\": \"all-MiniLM-L6-v2\",\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"num_train_epochs\": 4,\n",
    "        \"per_device_train_batch_size\": 16,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"weight_decay\": 0.01,\n",
    "    }\n",
    "\n",
    "    # Se config não for fornecido, usar padrões\n",
    "    if config is None:\n",
    "        config = default_config\n",
    "    else:\n",
    "        # Preencher valores faltantes com padrões\n",
    "        for key, value in default_config.items():\n",
    "            if key not in config:\n",
    "                config[key] = value\n",
    "\n",
    "    # Inicializar WandB\n",
    "    run = wandb.init(\n",
    "        project=\"sbert-similarity-optimization\",\n",
    "        config=config,\n",
    "        tags=[\"sentence-similarity\", \"sbert\", \"fine-tuning\"],\n",
    "    )\n",
    "\n",
    "    print(\"Iniciando treinamento com configurações:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    # Configurar modelo\n",
    "    model = SentenceTransformer(config[\"model_name\"])\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Configurar loss function\n",
    "    train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "\n",
    "    # Configurar avaliador\n",
    "    dev_evaluator = EmbeddingSimilarityEvaluator(\n",
    "        sentences1=[example.texts[0] for example in val_examples],\n",
    "        sentences2=[example.texts[1] for example in val_examples],\n",
    "        scores=[example.label for example in val_examples],\n",
    "        main_similarity=SimilarityFunction.COSINE,\n",
    "        show_progress_bar=True,\n",
    "        name=\"validation\",\n",
    "    )\n",
    "\n",
    "    return model, train_loss, dev_evaluator, run, config\n",
    "\n",
    "\n",
    "print(\"Função de configuração do modelo definida!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento do Modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Função para executar um experimento de treinamento\n",
    "def train_experiment(config=None):\n",
    "    \"\"\"\n",
    "    Executa um experimento completo de treinamento\n",
    "    \"\"\"\n",
    "\n",
    "    # Configurar modelo e componentes\n",
    "    model, train_loss, dev_evaluator, wandb_run, final_config = train_model_with_config(\n",
    "        config\n",
    "    )\n",
    "\n",
    "    # Configurar argumentos de treinamento\n",
    "    args = SentenceTransformerTrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs=final_config[\"num_train_epochs\"],\n",
    "        per_device_train_batch_size=final_config[\"per_device_train_batch_size\"],\n",
    "        per_device_eval_batch_size=final_config[\"per_device_train_batch_size\"],\n",
    "        learning_rate=final_config[\"learning_rate\"],\n",
    "        warmup_ratio=final_config[\"warmup_ratio\"],\n",
    "        weight_decay=final_config[\"weight_decay\"],\n",
    "        fp16=True,\n",
    "        dataloader_drop_last=False,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=50,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=50,\n",
    "        save_total_limit=2,\n",
    "        logging_steps=25,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_validation_spearman_cosine\",\n",
    "        greater_is_better=True,\n",
    "        run_name=f\"sbert-{final_config['model_name'].replace('/', '-')}-lr{final_config['learning_rate']:.1e}\",\n",
    "        report_to=\"wandb\",  # Importante: enviar métricas para WandB\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "\n",
    "    # Configurar trainer\n",
    "    trainer = SentenceTransformerTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        loss=train_loss,\n",
    "        evaluator=dev_evaluator,\n",
    "    )\n",
    "\n",
    "    # Treinar o modelo\n",
    "    trainer.train()\n",
    "\n",
    "    # Avaliar no conjunto de validação\n",
    "    val_score = dev_evaluator(model)\n",
    "    main_score = val_score[dev_evaluator.primary_metric]\n",
    "\n",
    "    # Registrar métricas adicionais no WandB\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"final_validation_score\": main_score,\n",
    "            \"model_name\": final_config[\"model_name\"],\n",
    "            \"total_parameters\": sum(p.numel() for p in model.parameters()),\n",
    "            \"trainable_parameters\": sum(\n",
    "                p.numel() for p in model.parameters() if p.requires_grad\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"Validation Score Final: {main_score:.4f}\")\n",
    "\n",
    "    # Finalizar WandB run\n",
    "    wandb.finish()\n",
    "\n",
    "    return model, main_score, final_config\n",
    "\n",
    "\n",
    "print(\"Função de treinamento configurada!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPÇÃO 1: Treinamento único com configurações customizadas\n",
    "def single_training():\n",
    "    \"\"\"Execute um único treinamento com configurações otimizadas\"\"\"\n",
    "\n",
    "    # Configurações melhoradas baseadas em boas práticas\n",
    "    optimized_config = {\n",
    "        \"model_name\": \"all-mpnet-base-v2\",  # Modelo mais potente\n",
    "        \"learning_rate\": 1e-5,  # Learning rate menor para fine-tuning\n",
    "        \"num_train_epochs\": 3,\n",
    "        \"per_device_train_batch_size\": 16,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"weight_decay\": 0.01,\n",
    "    }\n",
    "\n",
    "    print(\"=== EXECUTANDO TREINAMENTO ÚNICO ===\")\n",
    "    model, score, config = train_experiment(optimized_config)\n",
    "    return model, score, config\n",
    "\n",
    "\n",
    "# OPÇÃO 2: Hyperparameter Sweep\n",
    "def run_hyperparameter_sweep(count=10):\n",
    "    \"\"\"Execute um sweep de hyperparâmetros\"\"\"\n",
    "\n",
    "    print(\"=== EXECUTANDO HYPERPARAMETER SWEEP ===\")\n",
    "    print(f\"Número de experimentos: {count}\")\n",
    "\n",
    "    # Inicializar sweep\n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"sbert-similarity-optimization\")\n",
    "\n",
    "    def sweep_train():\n",
    "        # Esta função será chamada para cada experimento do sweep\n",
    "        config = wandb.config\n",
    "        train_experiment(dict(config))\n",
    "\n",
    "    # Executar sweep\n",
    "    wandb.agent(sweep_id, sweep_train, count=count)\n",
    "\n",
    "    print(\"Sweep concluído! Verifique os resultados no WandB.\")\n",
    "    return sweep_id\n",
    "\n",
    "\n",
    "print(\"Opções de treinamento configuradas!\")\n",
    "print(\"Execute uma das opções abaixo:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTAR TREINAMENTO\n",
    "# Escolha uma das opções abaixo:\n",
    "\n",
    "# Opção 1: Treinamento único otimizado (mais rápido)\n",
    "print(\"Iniciando treinamento único com configurações otimizadas...\")\n",
    "model, validation_score, final_config = single_training()\n",
    "\n",
    "# Opção 2: Hyperparameter Sweep (mais demorado, mas pode encontrar melhores parâmetros)\n",
    "# Descomente as linhas abaixo para executar o sweep:\n",
    "# print(\"Iniciando hyperparameter sweep...\")\n",
    "# sweep_id = run_hyperparameter_sweep(count=5)  # Reduzido para 5 experimentos\n",
    "\n",
    "print(\"\\nTreinamento concluído!\")\n",
    "print(f\"Score de validação: {validation_score:.4f}\")\n",
    "print(\"Configuração utilizada:\")\n",
    "for key, value in final_config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação do Modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Avaliação Detalhada do Modelo Treinado\n",
    "\n",
    "# Salvar o modelo treinado\n",
    "caminho_modelo = \"./fine-tuned-sbert-model\"\n",
    "model.save(caminho_modelo)\n",
    "print(\"Modelo salvo!\")\n",
    "\n",
    "# Inicializar WandB para avaliação (se ainda não estiver ativo)\n",
    "if not wandb.run:\n",
    "    wandb.init(\n",
    "        project=\"sbert-similarity-optimization\",\n",
    "        job_type=\"evaluation\",\n",
    "        tags=[\"evaluation\", \"final-model\"],\n",
    "    )\n",
    "\n",
    "print(\"\\nIniciando avaliação detalhada...\")\n",
    "\n",
    "# Gerar embeddings para as sentenças de validação\n",
    "val_embeddings1 = model.encode(\n",
    "    [example.texts[0] for example in val_examples],\n",
    "    show_progress_bar=True,\n",
    "    device=device,\n",
    "    batch_size=32,\n",
    ")\n",
    "val_embeddings2 = model.encode(\n",
    "    [example.texts[1] for example in val_examples],\n",
    "    show_progress_bar=True,\n",
    "    device=device,\n",
    "    batch_size=32,\n",
    ")\n",
    "\n",
    "# Calcular similaridade coseno\n",
    "similarities = util.cos_sim(val_embeddings1, val_embeddings2)\n",
    "val_predictions = [similarities[i][i] for i in range(len(similarities))]\n",
    "\n",
    "# Valores verdadeiros (escala original 0-5)\n",
    "val_true = np.array([example.label for example in val_examples])\n",
    "val_predictions_rescaled = np.array(val_predictions) * 5.0\n",
    "\n",
    "# Métricas\n",
    "mse = mean_squared_error(val_true, val_predictions_rescaled)\n",
    "mae = mean_absolute_error(val_true, val_predictions_rescaled)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Correlações\n",
    "pearson_corr = np.corrcoef(val_true, val_predictions_rescaled)[0, 1]\n",
    "spearman_corr, _ = spearmanr(val_true, val_predictions_rescaled)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"MÉTRICAS DE AVALIAÇÃO FINAL\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"MSE (0-5 scale): {mse:.4f}\")\n",
    "print(f\"MAE (0-5 scale): {mae:.4f}\")\n",
    "print(f\"RMSE (0-5 scale): {rmse:.4f}\")\n",
    "print(f\"Pearson Correlation: {pearson_corr:.4f}\")\n",
    "print(f\"Spearman Correlation: {spearman_corr:.4f}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Registrar métricas no WandB\n",
    "wandb.log(\n",
    "    {\n",
    "        \"final_mse\": mse,\n",
    "        \"final_mae\": mae,\n",
    "        \"final_rmse\": rmse,\n",
    "        \"final_pearson\": pearson_corr,\n",
    "        \"final_spearman\": spearman_corr,\n",
    "        \"validation_samples\": len(val_true),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Análise de distribuição por classe\n",
    "print(\"\\nAnálise por classe de similaridade:\")\n",
    "for score in sorted(df[\"similarity_score\"].unique()):\n",
    "    mask = val_true == score\n",
    "    if mask.sum() > 0:\n",
    "        predicted_for_class = val_predictions_rescaled[mask]\n",
    "        mean_pred = predicted_for_class.mean()\n",
    "        std_pred = predicted_for_class.std()\n",
    "        count = mask.sum()\n",
    "        print(\n",
    "            f\"Classe {score}: {count:3d} amostras - Pred: {mean_pred:.3f}±{std_pred:.3f}\"\n",
    "        )\n",
    "\n",
    "print(f\"\\nValidation Score Final: {spearman_corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teste no Conjunto de Teste\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Geração de Predições para o Conjunto de Teste\n",
    "\n",
    "caminho_submissao = \"./submission.csv\"\n",
    "\n",
    "print(f\"Lendo dados de teste de '{caminho_test}'...\")\n",
    "df_test = pd.read_csv(caminho_test)\n",
    "\n",
    "# Extrair as sentenças e os índices para o arquivo de submissão\n",
    "test_sentences1 = df_test[\"sentence1\"].tolist()\n",
    "test_sentences2 = df_test[\"sentence2\"].tolist()\n",
    "\n",
    "# Verificar se existe coluna de índice explícita ou usar o índice do DataFrame\n",
    "if \"index\" in df_test.columns:\n",
    "    test_indexes = df_test[\"index\"].tolist()\n",
    "elif \"id\" in df_test.columns:\n",
    "    test_indexes = df_test[\"id\"].tolist()\n",
    "else:\n",
    "    test_indexes = df_test.index.tolist()\n",
    "\n",
    "print(f\"Encontrados {len(df_test)} exemplos no conjunto de teste.\")\n",
    "print(\"Primeiras 3 linhas do teste:\")\n",
    "print(df_test.head(3))\n",
    "\n",
    "# Gerar Predições com otimizações\n",
    "print(\"\\nGerando embeddings e calculando similaridade...\")\n",
    "print(\"Utilizando batch processing otimizado...\")\n",
    "\n",
    "# Gerar embeddings com batch size otimizado\n",
    "batch_size = 64  # Aumentado para maior eficiência\n",
    "embeddings1 = model.encode(\n",
    "    test_sentences1,\n",
    "    show_progress_bar=True,\n",
    "    device=device,\n",
    "    batch_size=batch_size,\n",
    "    normalize_embeddings=True,  # Normalizar para melhor performance do cosseno\n",
    ")\n",
    "\n",
    "embeddings2 = model.encode(\n",
    "    test_sentences2,\n",
    "    show_progress_bar=True,\n",
    "    device=device,\n",
    "    batch_size=batch_size,\n",
    "    normalize_embeddings=True,\n",
    ")\n",
    "\n",
    "# Calcular similaridade de cosseno\n",
    "test_similarities = util.cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "# As predições são a diagonal da matriz de similaridade (score de 0-1)\n",
    "predictions_normalized = [\n",
    "    test_similarities[i][i].item() for i in range(len(test_similarities))\n",
    "]\n",
    "\n",
    "# Reescalonar para 0-5 e aplicar clipping\n",
    "predictions_rescaled = np.array(predictions_normalized) * 5.0\n",
    "predictions_rescaled = np.clip(predictions_rescaled, 0, 5)\n",
    "\n",
    "print(f\"Predições geradas: {len(predictions_rescaled)}\")\n",
    "print(\n",
    "    f\"Range das predições: [{predictions_rescaled.min():.3f}, {predictions_rescaled.max():.3f}]\"\n",
    ")\n",
    "print(f\"Média das predições: {predictions_rescaled.mean():.3f}\")\n",
    "\n",
    "# Criar DataFrame de submissão\n",
    "submission_df = pd.DataFrame(\n",
    "    {\"index\": test_indexes, \"predictions\": predictions_rescaled}\n",
    ")\n",
    "\n",
    "# Salvar arquivo de submissão\n",
    "submission_df.to_csv(caminho_submissao, index=False)\n",
    "print(f\"\\nArquivo de submissão salvo em: {caminho_submissao}\")\n",
    "\n",
    "# Registrar estatísticas no WandB\n",
    "if wandb.run:\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"test_samples\": len(predictions_rescaled),\n",
    "            \"test_pred_mean\": predictions_rescaled.mean(),\n",
    "            \"test_pred_std\": predictions_rescaled.std(),\n",
    "            \"test_pred_min\": predictions_rescaled.min(),\n",
    "            \"test_pred_max\": predictions_rescaled.max(),\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Verificação final\n",
    "print(\"\\nPrimeiras 10 linhas do arquivo de submissão:\")\n",
    "print(submission_df.head(10))\n",
    "\n",
    "print(\"\\nDistribuição das predições:\")\n",
    "print(submission_df[\"predictions\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Visualizar resultados\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# Gráfico 1: Scatter Plot (True vs Predicted na escala 0-1)\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(val_true, val_predictions, alpha=0.6)\n",
    "plt.plot([0, 1], [0, 1], \"r--\")\n",
    "plt.xlabel(\"True Similarity\")\n",
    "plt.ylabel(\"Predicted Similarity\")\n",
    "plt.title(\"Validação: True vs Predicted (0-1)\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Gráfico 2: Histogramas de Distribuição (Validação, na escala 0-1)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(val_predictions, bins=20, alpha=0.7, label=\"Predicted\", density=True)\n",
    "plt.hist(val_true, bins=20, alpha=0.7, label=\"True\", density=True)\n",
    "plt.xlabel(\"Similarity Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribuição - Validação (0-1)\")\n",
    "plt.legend()\n",
    "\n",
    "# Gráfico 3: Histograma de Distribuição (Teste, na escala 0-1)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(predictions_normalized, bins=20, alpha=0.7, color=\"green\")\n",
    "plt.xlabel(\"Similarity Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribuição - Teste (0-1)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Análise completa!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise Final e Recomendações para Melhoria\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RESUMO DO EXPERIMENTO E RECOMENDAÇÕES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Estatísticas do modelo atual\n",
    "print(f\"Modelo utilizado: {final_config.get('model_name', 'N/A')}\")\n",
    "print(f\"Score de validação: {validation_score:.4f}\")\n",
    "print(f\"Configuração final: {final_config}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ESTRATÉGIAS PARA MELHORAR O SCORE:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"1. MODELOS ALTERNATIVOS para testar:\")\n",
    "print(\"   - sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")\n",
    "print(\"   - sentence-transformers/all-roberta-large-v1\")\n",
    "print(\"   - sentence-transformers/stsb-roberta-large\")\n",
    "\n",
    "print(\"\\n2. TÉCNICAS DE ENSEMBLE:\")\n",
    "print(\"   - Combine predições de múltiplos modelos\")\n",
    "print(\"   - Use weighted averaging baseado na performance de validação\")\n",
    "\n",
    "print(\"\\n3. OTIMIZAÇÕES DE TREINAMENTO:\")\n",
    "print(\"   - Aumente num_train_epochs para 5-8\")\n",
    "print(\"   - Teste learning_rates menores (5e-6, 1e-6)\")\n",
    "print(\"   - Use gradient accumulation para batch sizes maiores\")\n",
    "\n",
    "print(\"\\n4. PRÉ-PROCESSAMENTO DOS DADOS:\")\n",
    "print(\"   - Limpeza mais agressiva de texto\")\n",
    "print(\"   - Normalização de caracteres especiais\")\n",
    "print(\"   - Remoção de stopwords específicas do domínio\")\n",
    "\n",
    "print(\"\\n5. ANÁLISE DOS DADOS:\")\n",
    "print(\"   - Verifique se há overfitting comparando train vs validation\")\n",
    "print(\"   - Analise exemplos onde o modelo erra mais\")\n",
    "print(\"   - Considere data augmentation\")\n",
    "\n",
    "print(\"\\n6. HYPERPARAMETER SWEEP COMPLETO:\")\n",
    "print(\"   - Execute sweep com mais experimentos (count=20-50)\")\n",
    "print(\"   - Teste diferentes combinações de modelo + learning rate\")\n",
    "\n",
    "if wandb.run:\n",
    "    print(\"\\n7. WANDB DASHBOARD:\")\n",
    "    print(f\"   - Acesse: https://wandb.ai/{wandb.run.entity}/{wandb.run.project}\")\n",
    "    print(\"   - Compare diferentes experimentos\")\n",
    "    print(\"   - Analise a evolução das métricas durante o treinamento\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Para executar o hyperparameter sweep, descomente as linhas\")\n",
    "print(\"na célula de treinamento e execute novamente!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13893386,
     "sourceId": 116386,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
